{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10845122,"sourceType":"datasetVersion","datasetId":6735354}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"8b5538d1-ffd5-4e45-87a8-81986e459791","cell_type":"markdown","source":"## Importação das bibliotecas","metadata":{}},{"id":"77773a93-0ae9-4973-9bf3-221ca4fe14d1","cell_type":"code","source":"!pip install git+https://github.com/openai/whisper.git git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:00:19.226038Z","iopub.execute_input":"2025-02-25T20:00:19.226379Z","iopub.status.idle":"2025-02-25T20:00:47.145815Z","shell.execute_reply.started":"2025-02-25T20:00:19.226351Z","shell.execute_reply":"2025-02-25T20:00:47.144744Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/whisper.git\n  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-eh4tdjcv\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-eh4tdjcv\n  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_opani86\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-_opani86\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.9.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\nCollecting triton>=2 (from openai-whisper==20240930)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (2.4.1)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->openai-whisper==20240930) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->openai-whisper==20240930) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper==20240930) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->openai-whisper==20240930) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->openai-whisper==20240930) (2024.2.0)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: openai-whisper, clip\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803668 sha256=92aae84c336776576d9a31ebb5473c80c830c4bb8e21711a8e556ffe4b74fc67\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_wpsfff9/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=5f56cc8450d2eb69fe234fce4b4160b35f7b225284ff0826d16801b99731b341\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_wpsfff9/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built openai-whisper clip\nInstalling collected packages: triton, ftfy, openai-whisper, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1 openai-whisper-20240930 triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"id":"b1758a25-3960-4227-911a-6674aecbcefd","cell_type":"code","source":"import os\nimport cv2\nfrom PIL import Image\nimport whisper\nimport torch\nimport clip\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:12:49.715281Z","iopub.execute_input":"2025-02-25T20:12:49.715671Z","iopub.status.idle":"2025-02-25T20:12:49.720802Z","shell.execute_reply.started":"2025-02-25T20:12:49.715642Z","shell.execute_reply":"2025-02-25T20:12:49.719751Z"}},"outputs":[],"execution_count":15},{"id":"dd4f9ad9-d439-49cd-a570-e69c15df0e02","cell_type":"code","source":"# Configurações globais\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nVIDEO_DIR = \"/kaggle/input/dataset-correlation/videos\"\nOUTPUT_DIR = \"/kaggle/working\"\nMETADATA_PATH = os.path.join(OUTPUT_DIR, \"metadata.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:01:06.080579Z","iopub.execute_input":"2025-02-25T20:01:06.081114Z","iopub.status.idle":"2025-02-25T20:01:06.085200Z","shell.execute_reply.started":"2025-02-25T20:01:06.081075Z","shell.execute_reply":"2025-02-25T20:01:06.084319Z"}},"outputs":[],"execution_count":3},{"id":"dc48e04f-f388-4580-ba9e-962e01232d73","cell_type":"code","source":"# Carregar modelos globalmente\nWHISPER_MODEL = whisper.load_model(\"base\", device=DEVICE)\nCLIP_MODEL, CLIP_PREPROCESS = clip.load(\"ViT-B/32\", device=DEVICE)\nBERT_TOKENIZER = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\nBERT_MODEL = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased').to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:01:06.086039Z","iopub.execute_input":"2025-02-25T20:01:06.086295Z","iopub.status.idle":"2025-02-25T20:01:41.605127Z","shell.execute_reply.started":"2025-02-25T20:01:06.086264Z","shell.execute_reply":"2025-02-25T20:01:41.603782Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 102MiB/s]\n/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 140MiB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a325be693f40bfb0ac8a097344a096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1542ccef778f4bc59d2b25342a5b61ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a5f9f65499745a895ee95c28245f113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288429c7d78041fb874267ed38a3033d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11985b8b30144493a63d05fff4392606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7190be1b3a4941f4a21586f9a468c31c"}},"metadata":{}}],"execution_count":4},{"id":"4ac744ec-856e-47f8-8622-28f7e686ca11","cell_type":"markdown","source":"## Processamento de Vídeo","metadata":{}},{"id":"838ee029-82ed-479d-93c2-9c5f6794a754","cell_type":"code","source":"def extract_frames(video_path, frame_rate=1):\n    \"\"\"Extrai frames de um vídeo a uma taxa específica\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(fps / frame_rate)\n    frame_count = 0\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_interval == 0:\n            frames.append(frame)\n        frame_count += 1\n    cap.release()\n    return frames\n\ndef transcribe_audio(video_path):\n    \"\"\"Transcreve o áudio do vídeo usando Whisper\"\"\"\n    result = WHISPER_MODEL.transcribe(video_path)\n    return result[\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:02:37.432690Z","iopub.execute_input":"2025-02-25T20:02:37.433041Z","iopub.status.idle":"2025-02-25T20:02:37.438703Z","shell.execute_reply.started":"2025-02-25T20:02:37.433010Z","shell.execute_reply":"2025-02-25T20:02:37.437653Z"}},"outputs":[],"execution_count":8},{"id":"8374127c-a369-4734-9101-9f71b895e9bd","cell_type":"markdown","source":"## Extração de Embeddings","metadata":{}},{"id":"b3709950-8a57-4f24-952e-22f5cb49937a","cell_type":"code","source":"def get_visual_embeddings(frames):\n    \"\"\"Gera embeddings visuais com CLIP (média dos frames)\"\"\"\n    embeddings = []\n    \n    for frame in frames:\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        pil_image = Image.fromarray(frame_rgb)\n        image_tensor = CLIP_PREPROCESS(pil_image).unsqueeze(0).to(DEVICE)\n        \n        with torch.no_grad():\n            embedding = CLIP_MODEL.encode_image(image_tensor)\n            embeddings.append(embedding.cpu().numpy().squeeze())\n    \n    return np.mean(embeddings, axis=0)  # Dimensão 512\n\ndef get_text_embeddings_bert(text):\n    \"\"\"Gera embeddings textuais com BERT (média dos tokens)\"\"\"\n    inputs = BERT_TOKENIZER(\n        text,\n        return_tensors='pt',\n        padding=True,\n        truncation=True,\n        max_length=512\n    ).to(DEVICE)\n    \n    with torch.no_grad():\n        outputs = BERT_MODEL(**inputs)\n    \n    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Dimensão 768","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:02:39.982210Z","iopub.execute_input":"2025-02-25T20:02:39.982559Z","iopub.status.idle":"2025-02-25T20:02:39.988583Z","shell.execute_reply.started":"2025-02-25T20:02:39.982533Z","shell.execute_reply":"2025-02-25T20:02:39.987656Z"}},"outputs":[],"execution_count":9},{"id":"e9e55a3b-ece8-4127-808a-fd3566e2bfad","cell_type":"code","source":"# Criar metadados (caso ainda não exista)\nvideos = [\n    (\"modified_Video1.mp4\", 0),\n    (\"modified_Video8.mp4\", 0)\n]\nfor i in range(2, 16):\n    if i != 8:\n        videos.append((f\"Video{i}.mp4\", 1))\n\nmetadata = pd.DataFrame(videos, columns=[\"video_path\", \"target\"])\nmetadata.to_csv(METADATA_PATH, index=False)\n\n# Carregar metadados\nmetadata = pd.read_csv(METADATA_PATH)\n\n# Processar cada vídeo\nall_data = []\n\nfor idx, row in metadata.iterrows():\n    video_path = os.path.join(VIDEO_DIR, row[\"video_path\"])\n    \n    if not os.path.exists(video_path):\n        print(f\"Vídeo não encontrado: {video_path}\")\n        continue\n    \n    # Processamento de vídeo\n    frames = extract_frames(video_path)\n    visual_embedding = get_visual_embeddings(frames)\n    \n    # Processamento de áudio\n    transcription = transcribe_audio(video_path)\n    text_embedding = get_text_embeddings_bert(transcription)\n    \n    # Combina embeddings\n    combined = np.concatenate([visual_embedding, text_embedding])  # 512 + 768 = 1280 dimensões\n    all_data.append({\n        \"features\": combined,\n        \"target\": row[\"target\"],\n        \"video_path\": row[\"video_path\"]\n    })\n    \n    print(f\"Processando: {row['video_path']}\")\n\n# Criar DataFrame final\ndf = pd.DataFrame(all_data)\n\n# Duplicar vídeos falsos até ter pelo menos 5 amostras (se necessário)\nfalse_samples = df[df['target'] == 0]\nif len(false_samples) < 5:\n    df = pd.concat([df, false_samples.sample(5 - len(false_samples), replace=True)])\n\n# Preparar features e rótulos\nX = np.vstack(df[\"features\"])\ny = df[\"target\"]\n\n# Balanceamento com SMOTE, ajustando k_neighbors\nsmote = SMOTE(k_neighbors=4, random_state=42)  # Ajustado para 4 vizinhos\nX_res, y_res = smote.fit_resample(X, y)\n\n# Divisão Treino-Teste\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n\nprint(\"Balanceamento concluído com sucesso!\")\nprint(f\"Tamanho do conjunto balanceado: {X_res.shape[0]} amostras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:06:28.618717Z","iopub.execute_input":"2025-02-25T20:06:28.619057Z","iopub.status.idle":"2025-02-25T20:07:11.683858Z","shell.execute_reply.started":"2025-02-25T20:06:28.619033Z","shell.execute_reply":"2025-02-25T20:07:11.679995Z"}},"outputs":[{"name":"stdout","text":"Processando: modified_Video1.mp4\nProcessando: modified_Video8.mp4\nProcessando: Video2.mp4\nProcessando: Video3.mp4\nProcessando: Video4.mp4\nProcessando: Video5.mp4\nProcessando: Video6.mp4\nProcessando: Video7.mp4\nProcessando: Video9.mp4\nProcessando: Video10.mp4\nProcessando: Video11.mp4\nProcessando: Video12.mp4\nProcessando: Video13.mp4\nProcessando: Video14.mp4\nProcessando: Video15.mp4\nBalanceamento concluído com sucesso!\nTamanho do conjunto balanceado: 26 amostras\n","output_type":"stream"}],"execution_count":11},{"id":"7811b993-a44d-4031-93e8-a72dea7e242a","cell_type":"code","source":"# Treinar XGBoost\nmodel = XGBClassifier(\n    objective='binary:logistic',\n    n_estimators=200,\n    max_depth=5,\n    learning_rate=0.01,\n    random_state=42\n)\nmodel.fit(X_train, y_train)\n\n# Avaliação\ny_pred = model.predict(X_test)\nprint(\"\\n=== Relatório de Classificação ===\")\nprint(classification_report(y_test, y_pred))\nprint(\"AUC-ROC:\", roc_auc_score(y_test, y_pred))\n\n# Ajuste de Threshold (opcional)\ny_probs = model.predict_proba(X_test)[:, 1]\noptimal_threshold = 0.4  # Ajuste conforme necessário (ex.: via curva ROC)\ny_pred_adj = (y_probs > optimal_threshold).astype(int)\nprint(\"\\n=== Relatório com Threshold Ajustado (0.4) ===\")\nprint(classification_report(y_test, y_pred_adj))\nprint(\"AUC-ROC (ajustado):\", roc_auc_score(y_test, y_pred_adj))\n\n# Salvar resultados\nresults_df = pd.DataFrame({\n    \"video_path\": df[\"video_path\"],\n    \"target\": df[\"target\"],\n    \"features\": list(X)  # embeddings combinados\n})\nresults_df.to_csv(os.path.join(OUTPUT_DIR, \"results_with_features.csv\"), index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:08:54.394328Z","iopub.execute_input":"2025-02-25T20:08:54.394892Z","iopub.status.idle":"2025-02-25T20:08:54.632059Z","shell.execute_reply.started":"2025-02-25T20:08:54.394858Z","shell.execute_reply":"2025-02-25T20:08:54.631033Z"}},"outputs":[{"name":"stdout","text":"\n=== Relatório de Classificação ===\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         3\n           1       1.00      1.00      1.00         3\n\n    accuracy                           1.00         6\n   macro avg       1.00      1.00      1.00         6\nweighted avg       1.00      1.00      1.00         6\n\nAUC-ROC: 1.0\n\n=== Relatório com Threshold Ajustado (0.4) ===\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         3\n           1       1.00      1.00      1.00         3\n\n    accuracy                           1.00         6\n   macro avg       1.00      1.00      1.00         6\nweighted avg       1.00      1.00      1.00         6\n\nAUC-ROC (ajustado): 1.0\n","output_type":"stream"}],"execution_count":12},{"id":"f1b6316e-562d-4f97-89e5-c0a960bf7d01","cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_probs)\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(f\"Threshold ótimo: {optimal_threshold}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:09:29.964916Z","iopub.execute_input":"2025-02-25T20:09:29.965204Z","iopub.status.idle":"2025-02-25T20:09:29.972940Z","shell.execute_reply.started":"2025-02-25T20:09:29.965182Z","shell.execute_reply":"2025-02-25T20:09:29.971680Z"}},"outputs":[{"name":"stdout","text":"Threshold ótimo: 0.8626664280891418\n","output_type":"stream"}],"execution_count":13},{"id":"1d04f7b3-a529-4c73-800a-166676f35855","cell_type":"markdown","source":"## Carregamento de metadados com target (Luan analise essa parte para combinar com a extracão de embbedings)","metadata":{}},{"id":"c3e65b0f-8d8d-4a62-bf4f-6a1b4a8adb5d","cell_type":"code","source":"from collections import Counter\nprint(\"Distribuição antes do SMOTE:\", Counter(y))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:09:51.895647Z","iopub.execute_input":"2025-02-25T19:09:51.895950Z","iopub.status.idle":"2025-02-25T19:09:51.913412Z","shell.execute_reply.started":"2025-02-25T19:09:51.895926Z","shell.execute_reply":"2025-02-25T19:09:51.912347Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-5f8fa8f07724>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distribuição antes do SMOTE:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"],"ename":"NameError","evalue":"name 'y' is not defined","output_type":"error"}],"execution_count":14},{"id":"2989ff1b-f878-4453-8659-161546de7ba6","cell_type":"code","source":"df = pd.DataFrame(X)\ndf['target'] = y\n\n# Duplica os vídeos falsos até ter pelo menos 5 amostras\nfalse_samples = df[df['target'] == 0]\ndf = pd.concat([df, false_samples.sample(3, replace=True)])  # Adiciona 3 cópias\n\nX_augmented = df.drop(columns=['target'])\ny_augmented = df['target']\n\nsmote = SMOTE(random_state=42, k_neighbors=1)\nX_res, y_res = smote.fit_resample(X_augmented, y_augmented)","metadata":{},"outputs":[],"execution_count":null},{"id":"e318094d-119d-4cbf-bc09-57b36417a569","cell_type":"code","source":"# Carrega metadados (rótulos manuais)\nmetadata = pd.read_csv(\"data/dataset/metadata.csv\")  # Arquivo com colunas: [video_path, target]\n\n# Processa cada vídeo\nall_data = []\n\nfor idx, row in metadata.iterrows():\n    video_path = os.path.join(VIDEO_DIR, row[\"video_path\"])\n    \n    # Processamento de vídeo\n    frames = extract_frames(video_path)\n    visual_embedding = get_visual_embeddings(frames)\n    \n    # Processamento de áudio\n    transcription = transcribe_audio(video_path)\n    text_embedding = get_text_embeddings_bert(transcription)\n    \n    # Combina embeddings\n    combined = np.concatenate([visual_embedding, text_embedding])\n    all_data.append({\n        \"features\": combined,\n        \"target\": row[\"target\"]\n    })\n\n    print(\"Em processamento\")\n\n# Cria DataFrame final\ndf = pd.DataFrame(all_data)\nfalse_samples = df[df['target'] == 0]\ndf = pd.concat([df, false_samples.sample(5, replace=True)])\nX = np.vstack(df[\"features\"])\ny = df[\"target\"]\n\n# Balanceamento com SMOTE\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X, y)\n\n# Treino-Teste Split\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2)\n\n# Treina XGBoost\nmodel = XGBClassifier(\n    objective='binary:logistic',\n    n_estimators=200,\n    max_depth=5,\n    learning_rate=0.01\n)\nmodel.fit(X_train, y_train)\n\n# Avaliação\ny_pred = model.predict(X_test)\nprint(\"Relatório de Classificação:\\n\", classification_report(y_test, y_pred))\nprint(\"AUC-ROC:\", roc_auc_score(y_test, y_pred))\n\n# Ajuste de Threshold (Opcional)\ny_probs = model.predict_proba(X_test)[:, 1]\noptimal_threshold = 0.4  # Definir via curva ROC\ny_pred_adj = (y_probs > optimal_threshold).astype(int)","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Em processamento\n"]},{"name":"stderr","output_type":"stream","text":["/home/abraao/anaconda3/envs/multimodal-env/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n","  warnings.warn(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Relatório de Classificação:\n","               precision    recall  f1-score   support\n","\n","           0       0.75      1.00      0.86         3\n","           1       1.00      0.67      0.80         3\n","\n","    accuracy                           0.83         6\n","   macro avg       0.88      0.83      0.83         6\n","weighted avg       0.88      0.83      0.83         6\n","\n","AUC-ROC: 0.8333333333333333\n"]}],"execution_count":9},{"id":"f5c7cfd8-f2fc-4d91-a269-424730e3ed20","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"d12142bb-37df-4ada-a541-3b7336f6a1e2","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"id":"c7118db5-5f08-45df-a8bd-53fa2bd97597","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}