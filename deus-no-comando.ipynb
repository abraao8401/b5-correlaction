{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10776216,"sourceType":"datasetVersion","datasetId":6686047},{"sourceId":10776227,"sourceType":"datasetVersion","datasetId":6686058},{"sourceId":10795478,"sourceType":"datasetVersion","datasetId":6699809},{"sourceId":10845122,"sourceType":"datasetVersion","datasetId":6735354}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install yt_dlp pydub git+https://github.com/openai/whisper.git git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T00:53:09.550858Z","iopub.execute_input":"2025-02-25T00:53:09.551404Z","iopub.status.idle":"2025-02-25T00:53:27.261216Z","shell.execute_reply.started":"2025-02-25T00:53:09.551347Z","shell.execute_reply":"2025-02-25T00:53:27.259761Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/whisper.git\n  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-w_55qod0\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-w_55qod0\n  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-eghtq4ya\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-eghtq4ya\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: yt_dlp in /usr/local/lib/python3.10/dist-packages (2025.2.19)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.9.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\nRequirement already satisfied: triton>=2 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.2.0)\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper==20240930) (2.4.1)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->openai-whisper==20240930) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->openai-whisper==20240930) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper==20240930) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->openai-whisper==20240930) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->openai-whisper==20240930) (2024.2.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=7eaa2441592080aefce88ec16e258fb381ff4eff16ca5f73d74a2936bfa2890e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-gpqmm3pi/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport cv2\nimport whisper\nimport numpy as np\nfrom moviepy.editor import VideoFileClip\nfrom pathlib import Path\nfrom pydub import AudioSegment\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom PIL import Image\nimport clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T00:54:18.831523Z","iopub.execute_input":"2025-02-25T00:54:18.831877Z","iopub.status.idle":"2025-02-25T00:54:18.837886Z","shell.execute_reply.started":"2025-02-25T00:54:18.831850Z","shell.execute_reply":"2025-02-25T00:54:18.836454Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Caminhos dos diretórios\ndata_path = \"/kaggle/input/dataset-correlation/videos\"  # Diretório onde estão os vídeos\noutput_path = \"dataset_preprocessado\"\nframes_dir = os.path.join(output_path, \"frames\")\naudios_dir = os.path.join(output_path, \"audios\")\ntranscripts_dir = os.path.join(output_path, \"transcricoes\")\n\n# Criar diretórios de saída\nfor directory in [frames_dir, audios_dir, transcripts_dir]:\n    Path(directory).mkdir(parents=True, exist_ok=True)\n\ndef extract_frames(video_path, output_folder, frame_rate=1):\n    \"\"\"Extrai frames do vídeo a cada 'frame_rate' segundo.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    interval = fps * frame_rate  # Pular frames de acordo com a taxa\n    count = 0\n    frame_id = 0\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        if count % interval == 0:\n            frame_filename = os.path.join(output_folder, f\"frame_{frame_id:03d}.jpg\")\n            cv2.imwrite(frame_filename, frame)\n            frame_id += 1\n        \n        count += 1\n    \n    cap.release()\n    print(f\"Frames extraídos e salvos em {output_folder}\")\n\ndef extract_audio(video_path, audio_path):\n    \"\"\"Extrai o áudio do vídeo e salva como WAV.\"\"\"\n    video = VideoFileClip(video_path)\n    audio = video.audio\n    audio.write_audiofile(audio_path)\n    sound = AudioSegment.from_file(audio_path, format=\"mp4\")\n    sound.export(audio_path, format=\"wav\")\n\ndef transcribe_audio(audio_path, transcript_path):\n    \"\"\"Gera a transcrição do áudio usando Whisper.\"\"\"\n    model = whisper.load_model(\"base\")\n    result = model.transcribe(audio_path)\n    with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(result[\"text\"])\n    print(f\"Transcrição salva em {transcript_path}\")\n\n# Processamento dos vídeos\nvideo_files = [f for f in os.listdir(data_path) if f.endswith(\".mp4\")]\n\nfor video_file in video_files:\n    video_path = os.path.join(data_path, video_file)\n    video_name = os.path.splitext(video_file)[0]\n    \n    # Criar pasta para os frames do vídeo\n    video_frames_dir = os.path.join(frames_dir, video_name)\n    Path(video_frames_dir).mkdir(parents=True, exist_ok=True)\n    \n    # Definir caminhos de saída\n    audio_path = os.path.join(audios_dir, f\"{video_name}.wav\")\n    transcript_path = os.path.join(transcripts_dir, f\"{video_name}.txt\")\n    \n    print(f\"Processando {video_file}...\")\n    extract_frames(video_path, video_frames_dir)\n    extract_audio(video_path, audio_path)\n    transcribe_audio(audio_path, transcript_path)\n    \nprint(\"Pré-processamento concluído!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T00:41:29.280433Z","iopub.execute_input":"2025-02-25T00:41:29.280840Z","iopub.status.idle":"2025-02-25T00:43:29.359040Z","shell.execute_reply.started":"2025-02-25T00:41:29.280813Z","shell.execute_reply":"2025-02-25T00:43:29.357761Z"}},"outputs":[{"name":"stdout","text":"Processando Video12.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video12\nMoviePy - Writing audio in dataset_preprocessado/audios/Video12.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\n","output_type":"stream"},{"name":"stderr","text":"100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 70.8MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Transcrição salva em dataset_preprocessado/transcricoes/Video12.txt\nProcessando Video15.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video15\nMoviePy - Writing audio in dataset_preprocessado/audios/Video15.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video15.txt\nProcessando Video7.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video7\nMoviePy - Writing audio in dataset_preprocessado/audios/Video7.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video7.txt\nProcessando Video13.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video13\nMoviePy - Writing audio in dataset_preprocessado/audios/Video13.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video13.txt\nProcessando Video4.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video4\nMoviePy - Writing audio in dataset_preprocessado/audios/Video4.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video4.txt\nProcessando Video6.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video6\nMoviePy - Writing audio in dataset_preprocessado/audios/Video6.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video6.txt\nProcessando modified_Video1.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/modified_Video1\nMoviePy - Writing audio in dataset_preprocessado/audios/modified_Video1.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/modified_Video1.txt\nProcessando modified_Video8.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/modified_Video8\nMoviePy - Writing audio in dataset_preprocessado/audios/modified_Video8.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/modified_Video8.txt\nProcessando Video5.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video5\nMoviePy - Writing audio in dataset_preprocessado/audios/Video5.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video5.txt\nProcessando Video11.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video11\nMoviePy - Writing audio in dataset_preprocessado/audios/Video11.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video11.txt\nProcessando Video3.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video3\nMoviePy - Writing audio in dataset_preprocessado/audios/Video3.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video3.txt\nProcessando Video14.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video14\nMoviePy - Writing audio in dataset_preprocessado/audios/Video14.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video14.txt\nProcessando Video10.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video10\nMoviePy - Writing audio in dataset_preprocessado/audios/Video10.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video10.txt\nProcessando Video9.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video9\nMoviePy - Writing audio in dataset_preprocessado/audios/Video9.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video9.txt\nProcessando Video2.mp4...\nFrames extraídos e salvos em dataset_preprocessado/frames/Video2\nMoviePy - Writing audio in dataset_preprocessado/audios/Video2.wav\n","output_type":"stream"},{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nTranscrição salva em dataset_preprocessado/transcricoes/Video2.txt\nPré-processamento concluído!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Caminhos dos dados preprocessados\nTRANSCRIPTIONS_PATH = \"/kaggle/working/dataset_preprocessado/transcricoes\"\nFRAMES_PATH = \"/kaggle/working/dataset_preprocessado/frames\"\nOUTPUT_EMBEDDINGS_PATH = \"/kaggle/working/dataset_preprocessado/embeddings\"\n\n# Criar diretório para armazenar embeddings\nPath(OUTPUT_EMBEDDINGS_PATH).mkdir(parents=True, exist_ok=True)\n\n# Carregar modelos pré-treinados\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device).eval()\n\ndef extract_text_embedding(text, model, tokenizer):\n    \"\"\"Extrai embeddings textuais usando BERT.\"\"\"\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n\ndef extract_image_embedding(image_path, model):\n    \"\"\"Extrai embeddings visuais usando CLIP.\"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    image = clip_preprocess(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        image_features = model.encode_image(image)\n    return image_features.squeeze().cpu().numpy()\n\n# Processar transcrições e extrair embeddings textuais\ntext_embeddings = {}\nfor file in os.listdir(TRANSCRIPTIONS_PATH):\n    if file.endswith(\".txt\"):\n        video_name = file.replace(\".txt\", \"\")\n        with open(os.path.join(TRANSCRIPTIONS_PATH, file), \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n        embedding = extract_text_embedding(text, bert_model, tokenizer)\n        np.save(os.path.join(OUTPUT_EMBEDDINGS_PATH, f\"{video_name}_text.npy\"), embedding)\n        text_embeddings[video_name] = embedding\n\n# Processar frames e extrair embeddings visuais\nimage_embeddings = {}\nfor video_folder in os.listdir(FRAMES_PATH):\n    video_path = os.path.join(FRAMES_PATH, video_folder)\n    if os.path.isdir(video_path):\n        embeddings_list = []\n        for img_file in sorted(os.listdir(video_path)):\n            if img_file.endswith(\".jpg\"):\n                img_path = os.path.join(video_path, img_file)\n                embedding = extract_image_embedding(img_path, clip_model)\n                embeddings_list.append(embedding)\n        video_embedding = np.mean(embeddings_list, axis=0)  # Média dos embeddings do vídeo\n        np.save(os.path.join(OUTPUT_EMBEDDINGS_PATH, f\"{video_folder}_image.npy\"), video_embedding)\n        image_embeddings[video_folder] = video_embedding\n\nprint(\"Extração de embeddings concluída!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T00:56:33.053936Z","iopub.execute_input":"2025-02-25T00:56:33.055364Z","iopub.status.idle":"2025-02-25T00:57:33.544336Z","shell.execute_reply.started":"2025-02-25T00:56:33.055257Z","shell.execute_reply":"2025-02-25T00:57:33.542953Z"}},"outputs":[{"name":"stdout","text":"Extração de embeddings concluída!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.stats import pearsonr\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T01:08:00.377828Z","iopub.execute_input":"2025-02-25T01:08:00.378617Z","iopub.status.idle":"2025-02-25T01:08:00.386195Z","shell.execute_reply.started":"2025-02-25T01:08:00.378568Z","shell.execute_reply":"2025-02-25T01:08:00.384045Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Caminho dos embeddings\nEMBEDDINGS_PATH = \"/kaggle/working/dataset_preprocessado/embeddings\"\n\n# Dicionários para armazenar os embeddings\ntext_embeddings = {}\nimage_embeddings = {}\n\n# Carregar embeddings textuais\nfor file in os.listdir(EMBEDDINGS_PATH):\n    if file.endswith(\"_text.npy\"):\n        video_name = file.replace(\"_text.npy\", \"\")\n        text_embeddings[video_name] = np.load(os.path.join(EMBEDDINGS_PATH, file))\n\n# Carregar embeddings visuais\nfor file in os.listdir(EMBEDDINGS_PATH):\n    if file.endswith(\"_image.npy\"):\n        video_name = file.replace(\"_image.npy\", \"\")\n        image_embeddings[video_name] = np.load(os.path.join(EMBEDDINGS_PATH, file))\n\nprint(f\"Carregados {len(text_embeddings)} embeddings textuais e {len(image_embeddings)} embeddings visuais.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T01:08:04.316966Z","iopub.execute_input":"2025-02-25T01:08:04.317379Z","iopub.status.idle":"2025-02-25T01:08:04.332642Z","shell.execute_reply.started":"2025-02-25T01:08:04.317342Z","shell.execute_reply":"2025-02-25T01:08:04.331276Z"}},"outputs":[{"name":"stdout","text":"Carregados 15 embeddings textuais e 15 embeddings visuais.\n","output_type":"stream"}],"execution_count":21}]}